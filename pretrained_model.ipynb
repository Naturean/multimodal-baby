{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multimodal.multimodal_lit import MultiModalLitModel\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_ENDPOINT\"]=\"https://hf-mirror.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = 'D:\\\\VSCode Projects\\\\jupyter\\\\Big Data and Multi-modal Data Fusion'\n",
    "DATASET_PATH = 'dataset'\n",
    "VIDEO_PATH = 'video'\n",
    "SCREENSHOT_PATH ='screenshots'\n",
    "TRANSCRIPT_PATH = 'merged_transcripts.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time process function\n",
    "def time_process(t: str):\n",
    "    t = t.strip()\n",
    "    if '-' in t:\n",
    "        t1, t2 = t.split('-')\n",
    "        t = (time_process(t1) + time_process(t2)) // 2\n",
    "    else:\n",
    "        t = t.split(':')\n",
    "        t = (int(t[0]) * 60 if t[0].isdigit() else 0) + (int(t[1]) if len(t) > 1 and t[1].isdigit() else 0)\n",
    "    return t\n",
    "\n",
    "# process image\n",
    "def process_image(img_path: str):\n",
    "    assert os.path.exists(img_path), f\"Image '{img_path}' does not exist\"\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = np.array(img)\n",
    "    assert img.shape == (480, 640, 3), f\"Image shape is {img.shape}, not (480, 640, 3).\"\n",
    "    return img\n",
    "\n",
    "# process utterance\n",
    "def process_utterance(utterance: str):\n",
    "    utterance = utterance.lower()\n",
    "    utterance = re.sub(r'[^\\w\\s]', '', utterance)\n",
    "    utterance = [word for word in utterance.strip().split(' ') if word != '']\n",
    "    return utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(os.path.join(BASE_PATH, TRANSCRIPT_PATH))\n",
    "# df = df[df['Speaker'].isin(['S', 'sam', 'Sam', 'Video (S)', 'child']) & df['Child Age']].reset_index(drop=True)\n",
    "\n",
    "images_labels = []\n",
    "texts_labels = []\n",
    "for i in tqdm(range(len(df)), desc='Processing images and texts'):\n",
    "    video_name = df.at[i, 'Video Name']\n",
    "    time = time_process(str(df.at[i, 'Time']))\n",
    "    screeshot_path = os.path.join(BASE_PATH, SCREENSHOT_PATH, video_name.split('.')[0], str(time) + '.jpg')\n",
    "    if not os.path.exists(screeshot_path): continue\n",
    "    # images_labels.append(process_image(screeshot_path))\n",
    "\n",
    "    utterance = df.at[i, 'Utterance']\n",
    "    [texts_labels.append(text) for text in process_utterance(utterance)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_labels.append(process_image(os.path.join(BASE_PATH, SCREENSHOT_PATH, 'S_20130901_1015_01\\\\743.jpg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "cvcl, preprocess = MultiModalLitModel.load_model(model_name=\"cvcl\")\n",
    "cvcl = cvcl.to(device)\n",
    "cvcl.eval()\n",
    "\n",
    "# create random image to encode\n",
    "# image_labels = torch.rand(4, 3, 224, 224)\n",
    "images_labels = torch.tensor(np.array(images_labels), dtype=torch.float32)\n",
    "images_labels = images_labels.permute(0, 3, 1, 2)\n",
    "images = images_labels.to(device)\n",
    "image_features = cvcl.encode_image(images)\n",
    "\n",
    "# create texts to encode\n",
    "# texts_labels = [\"ball\", \"puzzle\", \"car\"]\n",
    "texts_labels = list(dict.fromkeys(texts_labels))\n",
    "texts, texts_len = cvcl.tokenize(texts_labels)\n",
    "texts, texts_len = texts.to(device), texts_len.to(device)\n",
    "texts_features = cvcl.encode_text(texts, texts_len)\n",
    "\n",
    "# get logits from a batch of images and texts\n",
    "logits_per_image, logits_per_text = cvcl(images, texts, texts_len)\n",
    "\n",
    "probs = logits_per_image.softmax(dim=-1).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "combined_features = np.concatenate((texts_features.detach().cpu().numpy(), image_features.detach().cpu().numpy()), axis=0)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "combined_features_2d = tsne.fit_transform(combined_features)\n",
    "\n",
    "texts_features_2d = combined_features_2d[:len(texts_features)]\n",
    "image_features_2d = combined_features_2d[len(texts_features):]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(texts_features_2d[:, 0], texts_features_2d[:, 1], c='r', label='Text Features', s=5)\n",
    "plt.scatter(image_features_2d[:, 0], image_features_2d[:, 1], c='b', label='Image Features', s=5)\n",
    "\n",
    "plt.title('Text and Image Features in 2D TSNE Space')\n",
    "plt.xlabel('TSNE 1')\n",
    "plt.ylabel('TSNE 2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "top_k_indices = np.unravel_index(np.argsort(probs, axis=None)[-k:], probs.shape)\n",
    "\n",
    "image_indices = top_k_indices[0]\n",
    "text_indices = top_k_indices[1]\n",
    "\n",
    "for i in range(k - 1, -1, -1):\n",
    "    _image = df.at[image_indices[i], 'Video Name'].split('.')[0] + '/' + str(time_process(df.at[image_indices[i], 'Time'])) + '.jpg'\n",
    "    _text = texts_labels[text_indices[i]]\n",
    "    print(f\"[{k - i}] image: {_image}, text: {_text}, prob: {probs[image_indices[i], text_indices[i]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5 * k))\n",
    "for i in range(k - 1, -1, -1):\n",
    "    image_path = os.path.join(BASE_PATH, SCREENSHOT_PATH, df.at[image_indices[i], 'Video Name'].split('.')[0], str(time_process(df.at[image_indices[i], 'Time'])) + '.jpg')\n",
    "    img = Image.open(image_path)\n",
    "    text_label = f\"[{k - i}] text: {texts_labels[text_indices[i]]}, prob: {probs[image_indices[i], text_indices[i]]}\"\n",
    "\n",
    "    plt.subplot(k, 1, k - i)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(text_label)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_indices = np.unique(image_indices)\n",
    "text_indices = np.unique(text_indices)\n",
    "probs_matrix = np.zeros((len(image_indices), len(text_indices)))\n",
    "\n",
    "for i, image_index in enumerate(image_indices):\n",
    "    for j, text_index in enumerate(text_indices):\n",
    "        probs_matrix[i, j] = probs[image_index, text_index]\n",
    "\n",
    "text_labels = [texts_labels[i] for i in text_indices]\n",
    "image_labels = [df.at[i, 'Video Name'].split('.')[0] + '/' + str(time_process(df.at[i, 'Time'])) + '.jpg' for i in image_indices]\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.heatmap(probs_matrix, annot=True, cmap='Blues', xticklabels=text_labels, yticklabels=image_labels, vmax=1.0, vmin=0.0, fmt='.5f')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal-baby",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
